{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import cPickle, gzip\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "import seq2seq_model\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import parser.Smipar as Smipar\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99, \"Learning rate decays by this much.\")\n",
    "flags.DEFINE_float(\"max_gradient_norm\", 5.0, \"Clip gradients to this norm.\")\n",
    "flags.DEFINE_integer(\"batch_size\", 1, \"Batch size to use during training.\")\n",
    "flags.DEFINE_integer(\"size\", 600, \"Size of each model layer.\")\n",
    "flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n",
    "flags.DEFINE_integer(\"reactant_vocab_size\", 311, \"Reactant vocabulary size.\")\n",
    "flags.DEFINE_integer(\"product_vocab_size\", 180, \"Product vocabulary size.\")\n",
    "flags.DEFINE_string(\"train_dir\", \"checkpoint/saved_models\", \"Training dir.\")\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "_buckets = [(54, 54), (70, 60), (90, 65), (150, 80)]\n",
    "\n",
    "with gzip.open('data/vocab/vocab_list.pkl.gz', 'rb') as list_file:\n",
    "    reactants_token_list, products_token_list = cPickle.load(list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel( \\\n",
    "        FLAGS.reactant_vocab_size, FLAGS.product_vocab_size, _buckets, \\\n",
    "        FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size, \\\n",
    "        FLAGS.learning_rate, FLAGS.learning_rate_decay_factor, forward_only=forward_only)\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.initialize_all_variables())\n",
    "    return model\n",
    "\n",
    "def cano(smiles): # canonicalize smiles by MolToSmiles function\n",
    "    return Chem.MolToSmiles(Chem.MolFromSmiles(smiles)) if (smiles != '') else ''\n",
    "\n",
    "def fp_sim(smiles1, smiles2): # fingerprint similarity (Tanimoto)\n",
    "    try:\n",
    "        fp1 = FingerprintMols.FingerprintMol(Chem.MolFromSmiles(smiles1))\n",
    "        fp2 = FingerprintMols.FingerprintMol(Chem.MolFromSmiles(smiles2))\n",
    "        return DataStructs.FingerprintSimilarity(fp1, fp2)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def avg(l):\n",
    "    return sum(l)/float(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoint/saved_models/gen.ckpt-8\n",
      "11\t6.49795528998\t0.83520853025\t180\t32\n",
      "12\t12.9363915519\t0.616262961667\t26\t112\n",
      "13\t14.9942373492\t0.516243106783\t7\t150\n",
      "14\t15.8961106008\t0.480975144075\t1\t156\n",
      "15\t16.876896345\t0.475245438239\t0\t150\n",
      "16\t17.4883979243\t0.472577721873\t0\t152\n",
      "17\t17.3386820698\t0.521887936522\t0\t121\n"
     ]
    }
   ],
   "source": [
    "loss_list = [[] for _ in range(11, 18)]\n",
    "sim_list = [[] for _ in range(11, 18)]\n",
    "products_list = [[] for _ in range(11, 18)]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = create_model(sess, True)\n",
    "    \n",
    "    for i, j in enumerate(range(11, 18)):\n",
    "        with open('gen_rxn(test)/data/all_rxns_' + str(j) + '.txt') as f:\n",
    "            rsmi_list = f.read().splitlines()\n",
    "        \n",
    "        for rsmi in rsmi_list: # list of RSMIs to be tested\n",
    "\n",
    "            reactant_list = []\n",
    "            agent_list = []\n",
    "            product_list = []\n",
    "\n",
    "            split_rsmi = rsmi.split('>')\n",
    "            reactants = cano(split_rsmi[0]).split('.')\n",
    "            agents = cano(split_rsmi[1]).split('.')\n",
    "            products = cano(split_rsmi[2]).split('.')\n",
    "\n",
    "            for reactant in reactants:\n",
    "                reactant_list += Smipar.parser_list(reactant)\n",
    "                reactant_list += '.'\n",
    "            for agent in agents:\n",
    "                agent_list += Smipar.parser_list(agent)\n",
    "                agent_list += '.'\n",
    "            for product in products:\n",
    "                product_list += Smipar.parser_list(product)\n",
    "                product_list += '.'\n",
    "\n",
    "            reactant_list.pop() # to pop last '.'\n",
    "            agent_list.pop()\n",
    "            product_list.pop()\n",
    "\n",
    "            products.append(data_utils.EOS_ID)\n",
    "\n",
    "            reactant_list += '>'\n",
    "            reactant_list += agent_list\n",
    "\n",
    "            token_ids = [reactants_token_list.index(r) for r in reactant_list]\n",
    "            product_ids = [products_token_list.index(p) for p in product_list]\n",
    "\n",
    "            bucket_id = min([b for b in xrange(len(_buckets)) if _buckets[b][0] > len(token_ids)])\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch( \\\n",
    "                                       {bucket_id: [(token_ids, product_ids)]}, bucket_id)\n",
    "            _, loss, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                           target_weights, bucket_id, True)\n",
    "\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            if data_utils.EOS_ID in outputs:\n",
    "                outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "            products_ = ''.join([tf.compat.as_str(products_token_list[output]) for output in outputs])\n",
    "\n",
    "            loss_list[i].append(loss)\n",
    "            sim_list[i].append(fp_sim(split_rsmi[2], products_))\n",
    "            products_list[i].append(products_)\n",
    "            \n",
    "        print(j, avg(loss_list[i]), avg(sim_list[i]), sim_list[i].count(1), sim_list[i].count(0), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number, average loss(cross entropy), average tanimoto similarity, right, absolutely wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
